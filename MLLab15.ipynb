{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veerannanamana/ML-LAB/blob/main/MLLab15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a program to Implement Principle Component Analysis\n",
        "\n",
        "Principal Component Analysis: It is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation. These new transformed features are called the Principal Components.\n",
        "\n",
        "PCA generally tries to find the lower-dimensional surface to project the high-dimensional data.\n",
        "\n",
        "Some common terms used in PCA algorithm\n",
        "\n",
        "Dimensionality: It is the number of features or variables present in the given dataset. More easily, it is the number of columns present in the dataset.\n",
        "\n",
        "Correlation: It signifies that how strongly two variables are related to each other. Such as if one changes, the other variable also gets changed. The correlation value ranges from -1 to +1. Here, -1 occurs if variables are inversely proportional to each other, and +1 indicates that variables are directly proportional to each other.\n",
        "\n",
        "Orthogonal: It defines that variables are not correlated to each other, and hence the correlation between the pair of variables is zero.\n",
        "\n",
        "Eigenvectors: If there is a square matrix M, and a non-zero vector v is given. Then v will be eigenvector if Av is the scalar multiple of v. Covariance Matrix: A matrix containing the covariance between the pair of variables is called the Covariance Matrix.\n",
        "\n",
        "Steps for PCA algorithm\n",
        "\n",
        "1. Getting the dataset\n",
        "\n",
        "Firstly, we need to take the input dataset and divide it into two subparts X and Y, where X is the training set, and Y is the validation set.\n",
        "\n",
        "2. Representing data into a structure\n",
        "\n",
        "Now we will represent our dataset into a structure. Such as we will represent the two-dimensional matrix of independent variable X. Here each row corresponds to the data items, and the column corresponds to the Features. The number of columns is the dimensions of the dataset.\n",
        "\n",
        "3. Standardizing the data\n",
        "\n",
        "In this step, we will standardize our dataset. Such as in a particular column, the features with high variance are more important compared to the features with lower variance. If the importance of features is independent of the variance of the feature, then we will divide each data item in a column with the standard deviation of the column. Here we will name the matrix as Z.\n",
        "\n",
        "4. Calculating the Covariance of Z\n",
        "\n",
        "To calculate the covariance of Z, we will take the matrix Z, and will transpose it. After transpose, we will multiply it by Z. The output matrix will be the Covariance matrix of Z.\n",
        "\n",
        "5. Calculating the Eigen Values and Eigen Vectors\n",
        "\n",
        "Now we need to calculate the eigenvalues and eigenvectors for the resultant covariance matrix Z. Eigenvectors or the covariance matrix are the directions of the axes with high information. And the coefficients of these eigenvectors are defined as the eigenvalues.\n",
        "\n",
        "6. Sorting the Eigen Vectors\n",
        "\n",
        "In this step, we will take all the eigenvalues and will sort them in decreasing order, which means from largest to smallest. And simultaneously sort the eigenvectors accordingly in matrix P of eigenvalues. The resultant matrix will be named as P*.\n",
        "\n",
        "7. Calculating the new features Or Principal Components\n",
        "\n",
        "Here we will calculate the new features. To do this, we will multiply the P* matrix to the Z. In the resultant matrix Z, each observation is the linear combination of original features. Each column of the Z matrix is independent of each other.\n",
        "\n",
        "8. Remove less or unimportant features from the new dataset.\n",
        "\n",
        "The new feature set has occurred, so we will decide here what to keep and what to remove. It means, we will only keep the relevant or important features in the new dataset, and unimportant features will be removed out.\n",
        "\n",
        "Dataset Details (Wine Dataset): The Wine dataset consists of 178 instances with 13 features.\n",
        "\n",
        "The features represent the chemical analysis of wines grown in the same region in Italy but derived from three different cultivars (classes).\n",
        "\n",
        "Features include alcohol content, malic acid, ash, magnesium, flavonoids, and others."
      ],
      "metadata": {
        "id": "OPY6XhusheU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data  # Features\n",
        "y = wine.target  # Target (cultivars/classes)\n",
        "\n",
        "# Convert the dataset into a DataFrame for better readability\n",
        "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(df.head())\n",
        "\n",
        "# Standardize the dataset (mean=0, variance=1) - Important step for PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Implement PCA (reduce to 2 components for visualization)\n",
        "pca = sklearnPCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance to see how much information (variance) is retained\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"\\nExplained Variance (Percentage of Variance Explained by each Component):\")\n",
        "print(explained_variance)\n",
        "\n",
        "\n",
        "# Plot the PCA result\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='rainbow', edgecolor='k', s=100)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA - Wine Dataset (2 Components)')\n",
        "plt.colorbar(scatter, label='Class Label')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7QE9_OLHh1E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Details (Digits Dataset):\n",
        "\n",
        "The Digits dataset consists of 1,797 samples of handwritten digits, with 10 classes (0–9).\n",
        "\n",
        "Each sample is represented by 64 features (an 8x8 pixel image).\n",
        "\n",
        "The goal is to reduce the dimensionality using PCA while retaining most of the variance in the data."
      ],
      "metadata": {
        "id": "P-BY_4sgh7JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data  # Features (64 dimensions)\n",
        "y = digits.target  # Target (digit labels 0–9)\n",
        "\n",
        "# Dataset Overview\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Number of samples:\", X.shape[0])\n",
        "print(\"Number of features:\", X.shape[1])\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2 (for visualization)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"\\nExplained Variance (Percentage of Variance Explained by each Component):\")\n",
        "print(explained_variance)\n",
        "\n",
        "# Plot the PCA result\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='Spectral', edgecolor='k', s=50)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA - Digits Dataset (2 Components)')\n",
        "plt.colorbar(scatter, label='Digit Labels (0–9)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R2EdIrYdh786"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}